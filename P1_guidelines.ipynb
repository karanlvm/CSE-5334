{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 5334 Programming Assignment 1 (P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summer 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due: 11:59pm Central Time, Friday, July 5, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a toy \"search engine\" in Python. You code will read a corpus and produce TF-IDF vectors for documents in the corpus. Then, given a query string, you code will return the query answer--the document with the highest cosine similarity score for the query. \n",
    "\n",
    "The instructions on this assignment are written in an .ipynb file. You can use the following commands to install the Jupyter notebook viewer. You can use the following commands to install the Jupyter notebook viewer. \"pip\" is a command for installing Python packages. You are required to use Python 3.5.1 or more recent versions of Python in this project. \n",
    "\n",
    "    pip install jupyter\n",
    "\n",
    "    pip install notebook (You might have to use \"sudo\" if you are installing them at system level)\n",
    "\n",
    "To run the Jupyter notebook viewer, use the following command:\n",
    "\n",
    "    jupyter notebook P1.ipynb\n",
    "\n",
    "The above command will start a webservice at http://localhost:8888/ and display the instructions in the '.ipynb' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This assignment must be done individually. You must implement the whole assignment by yourself. Academic dishonety will have serious consequences.\n",
    "* You can discuss topics related to the assignment with your fellow students. But you are not allowed to discuss/share your solution and code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a corpus of 40 Inaugural addresses of different US presidents. We processed the corpus and provided you a .zip file, which includes 40 .txt files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You are required to submit a single .py file of your code.\n",
    "\n",
    "2. You are expected to use several modules in NLTK--a natural language processing toolkit for Python. NLTK doesn't come with Python by default. You need to install it and \"import\" it in your .py file. NLTK's website (http://www.nltk.org/index.html) provides a lot of useful information, including a book http://www.nltk.org/book/, as well as installation instructions (http://www.nltk.org/install.html).\n",
    "\n",
    "3. In programming assignment 1, other than NLTK, you are not allowed to use any other non-standard Python package. However, you are free to use anything from the the Python Standard Library that comes with Python (https://docs.python.org/3/library/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You code should accomplish the following tasks:\n",
    "\n",
    "(1) <b>Read</b> the 40 .txt files, each of which has the transcript of inaugural addresses by different US presidents. The following code does it. Make sure to replace \"corpusroot\" by your directory where the files are stored. In the example below, \"corpusroot\" is a sub-folder named \"US_Inaugural_Addresses\" in the folder containing the python file of the code. \n",
    "\n",
    "In this assignment we ignore the difference between lower and upper cases. So convert the text to lower case before you do anything else with the text. For a query, also convert it to lower case before you answer the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "corpusroot = './US_Inaugural_Addresses'\n",
    "file_count = 0\n",
    "\n",
    "for filename in os.listdir(corpusroot):\n",
    "    if filename.startswith('0') or filename.startswith('1') or filename.startswith('2') or filename.startswith('3') or filename.startswith('4'):\n",
    "        file = open(os.path.join(corpusroot, filename), \"r\", encoding='windows-1252')\n",
    "        doc = file.read()\n",
    "        file.close() \n",
    "        doc = doc.lower()\n",
    "        file_count += 1\n",
    "\n",
    "print(file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) <b>Tokenize</b> the content of each file. For this, you need a tokenizer. For example, the following piece of code uses a regular expression tokenizer to return all course numbers in a string. Play with it and edit it. You can change the regular expression and the string to observe different output results. \n",
    "\n",
    "For tokenizing the inaugural Presidential speeches, we will use RegexpTokenizer(r'[a-zA-Z]+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens from 01_washington_1789.txt: ['george', 'washington', 'fellow', 'citizens', 'of', 'the', 'senate', 'and', 'of', 'the']\n",
      "Tokens from 02_washington_1793.txt: ['george', 'washington', 'fellow', 'citizens', 'i', 'am', 'again', 'called', 'upon', 'by']\n",
      "Tokens from 03_adams_john_1797.txt: ['john', 'adams', 'when', 'it', 'was', 'first', 'perceived', 'in', 'early', 'times']\n",
      "Tokens from 04_jefferson_1801.txt: ['thomas', 'jefferson', 'friends', 'and', 'fellow', 'citizens', 'called', 'upon', 'to', 'undertake']\n",
      "Tokens from 05_jefferson_1805.txt: ['thomas', 'jefferson', 'proceeding', 'fellow', 'citizens', 'to', 'that', 'qualification', 'which', 'the']\n",
      "Tokens from 06_madison_1809.txt: ['james', 'madison', 'unwilling', 'to', 'depart', 'from', 'examples', 'of', 'the', 'most']\n",
      "Tokens from 07_madison_1813.txt: ['james', 'madison', 'about', 'to', 'add', 'the', 'solemnity', 'of', 'an', 'oath']\n",
      "Tokens from 08_monroe_1817.txt: ['james', 'monroe', 'i', 'should', 'be', 'destitute', 'of', 'feeling', 'if', 'i']\n",
      "Tokens from 09_monroe_1821.txt: ['james', 'monroe', 'fellow', 'citizens', 'i', 'shall', 'not', 'attempt', 'to', 'describe']\n",
      "Tokens from 10_adams_john_quincy_1825.txt: ['john', 'quincy', 'adams', 'in', 'compliance', 'with', 'an', 'usage', 'coeval', 'with']\n",
      "Tokens from 11_jackson_1829.txt: ['andrew', 'jackson', 'fellow', 'citizens', 'about', 'to', 'undertake', 'the', 'arduous', 'duties']\n",
      "Tokens from 12_jackson_1833.txt: ['andrew', 'jackson', 'the', 'will', 'of', 'the', 'american', 'people', 'expressed', 'through']\n",
      "Tokens from 13_van_buren_1837.txt: ['martin', 'van', 'buren', 'fellow', 'citizens', 'the', 'practice', 'of', 'all', 'my']\n",
      "Tokens from 14_harrison_1841.txt: ['william', 'henry', 'harrison', 'called', 'from', 'a', 'retirement', 'which', 'i', 'had']\n",
      "Tokens from 15_polk_1845.txt: ['james', 'k', 'polk', 'fellow', 'citizens', 'without', 'solicitation', 'on', 'my', 'part']\n",
      "Tokens from 16_taylor_1849.txt: ['zachary', 'taylor', 'elected', 'by', 'the', 'american', 'people', 'to', 'the', 'highest']\n",
      "Tokens from 17_pierce_1853.txt: ['franklin', 'pierce', 'my', 'countrymen', 'it', 'is', 'a', 'relief', 'to', 'feel']\n",
      "Tokens from 18_buchanan_1857.txt: ['james', 'buchanan', 'fellow', 'citizens', 'i', 'appear', 'before', 'you', 'this', 'day']\n",
      "Tokens from 19_lincoln_1861.txt: ['abraham', 'lincoln', 'fellow', 'citizens', 'of', 'the', 'united', 'states', 'in', 'compliance']\n",
      "Tokens from 20_lincoln_1865.txt: ['abraham', 'lincoln', 'fellow', 'countrymen', 'at', 'this', 'second', 'appearing', 'to', 'take']\n",
      "Tokens from 21_grant_1869.txt: ['ulysses', 's', 'grant', 'citizens', 'of', 'the', 'united', 'states', 'your', 'suffrages']\n",
      "Tokens from 22_grant_1873.txt: ['ulysses', 's', 'grant', 'fellow', 'citizens', 'under', 'providence', 'i', 'have', 'been']\n",
      "Tokens from 23_hayes_1877.txt: ['rutherford', 'b', 'hayes', 'fellow', 'citizens', 'we', 'have', 'assembled', 'to', 'repeat']\n",
      "Tokens from 24_garfield_1881.txt: ['james', 'a', 'garfield', 'fellow', 'citizens', 'we', 'stand', 'to', 'day', 'upon']\n",
      "Tokens from 25_cleveland_1885.txt: ['grover', 'cleveland', 'fellow', 'citizens', 'in', 'the', 'presence', 'of', 'this', 'vast']\n",
      "Tokens from 26_harrison_1889.txt: ['benjamin', 'harrison', 'fellow', 'citizens', 'there', 'is', 'no', 'constitutional', 'or', 'legal']\n",
      "Tokens from 27_cleveland_1893.txt: ['grover', 'cleveland', 'my', 'fellow', 'citizens', 'in', 'obedience', 'of', 'the', 'mandate']\n",
      "Tokens from 28_mckinley_1897.txt: ['william', 'mckinley', 'fellow', 'citizens', 'in', 'obedience', 'to', 'the', 'will', 'of']\n",
      "Tokens from 29_mckinley_1901.txt: ['william', 'mckinley', 'my', 'fellow', 'citizens', 'when', 'we', 'assembled', 'here', 'on']\n",
      "Tokens from 30_roosevelt_theodore_1905.txt: ['theodore', 'roosevelt', 'my', 'fellow', 'citizens', 'no', 'people', 'on', 'earth', 'have']\n",
      "Tokens from 31_taft_1909.txt: ['william', 'howard', 'taft', 'my', 'fellow', 'citizens', 'anyone', 'who', 'has', 'taken']\n",
      "Tokens from 32_wilson_1913.txt: ['woodrow', 'wilson', 'there', 'has', 'been', 'a', 'change', 'of', 'government', 'it']\n",
      "Tokens from 33_wilson_1917.txt: ['woodrow', 'wilson', 'the', 'four', 'years', 'which', 'have', 'elapsed', 'since', 'last']\n",
      "Tokens from 34_harding_1921.txt: ['warren', 'g', 'harding', 'when', 'one', 'surveys', 'the', 'world', 'about', 'him']\n",
      "Tokens from 35_coolidge_1925.txt: ['calvin', 'coolidge', 'my', 'countrymen', 'no', 'one', 'can', 'contemplate', 'current', 'conditions']\n",
      "Tokens from 36_hoover_1929.txt: ['herbert', 'hoover', 'delivered', 'in', 'person', 'at', 'the', 'capitol', 'my', 'countrymen']\n",
      "Tokens from 37_roosevelt_franklin_1933.txt: ['franklin', 'd', 'roosevelt', 'i', 'am', 'certain', 'that', 'my', 'fellow', 'americans']\n",
      "Tokens from 38_roosevelt_franklin_1937.txt: ['franklin', 'd', 'roosevelt', 'when', 'four', 'years', 'ago', 'we', 'met', 'to']\n",
      "Tokens from 39_roosevelt_franklin_1941.txt: ['franklin', 'd', 'roosevelt', 'on', 'each', 'national', 'day', 'of', 'inauguration', 'since']\n",
      "Tokens from 40_roosevelt_franklin_1945.txt: ['franklin', 'd', 'roosevelt', 'mr', 'chief', 'justice', 'mr', 'vice', 'president', 'my']\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "corpusroot = './US_Inaugural_Addresses'\n",
    "file_count = 0\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "for filename in os.listdir(corpusroot):\n",
    "    if filename.startswith('0') or filename.startswith('1') or filename.startswith('2') or filename.startswith('3') or filename.startswith('4'):\n",
    "        file = open(os.path.join(corpusroot, filename), \"r\", encoding='windows-1252')\n",
    "        doc = file.read()\n",
    "        doc = doc.lower()\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        print(f'Tokens from {filename}: {tokens[:10]}') #Printing only first 10 tokens \n",
    "        file.close() \n",
    "        file_count += 1\n",
    "print(file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Perform <b>stopword removal</b> on the obtained tokens. NLTK already comes with a stopword list, as a corpus in the \"NLTK Data\" (http://www.nltk.org/nltk_data/). You need to install this corpus. Follow the instructions at http://www.nltk.org/data.html. You can also find the instruction in this book: http://www.nltk.org/book/ch01.html (Section 1.2 Getting Started with NLTK). Basically, use the following statements in Python interpreter. A pop-up window will appear. Click \"Corpora\" and choose \"stopwords\" from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the stopword list is downloaded, you will find a file \"english\" in folder nltk_data/corpora/stopwords, where folder nltk_data is the download directory in the step above. The file contains 179 stopwords. nltk.corpus.stopwords will give you this list of stopwords. Try the following piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Also perform <b>stemming</b> on the obtained tokens. NLTK comes with a Porter stemmer. Try the following code and learn how to use the stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens from 01_washington_1789.txt: ['george', 'washington', 'fellow', 'citizens', 'of', 'the', 'senate', 'and', 'of', 'the']\n",
      "Stemmed tokens from 01_washington_1789.txt: ['georg', 'washington', 'fellow', 'citizen', 'of', 'the', 'senat', 'and', 'of', 'the']\n",
      "Tokens from 02_washington_1793.txt: ['george', 'washington', 'fellow', 'citizens', 'i', 'am', 'again', 'called', 'upon', 'by']\n",
      "Stemmed tokens from 02_washington_1793.txt: ['georg', 'washington', 'fellow', 'citizen', 'i', 'am', 'again', 'call', 'upon', 'by']\n",
      "Tokens from 03_adams_john_1797.txt: ['john', 'adams', 'when', 'it', 'was', 'first', 'perceived', 'in', 'early', 'times']\n",
      "Stemmed tokens from 03_adams_john_1797.txt: ['john', 'adam', 'when', 'it', 'wa', 'first', 'perceiv', 'in', 'earli', 'time']\n",
      "Tokens from 04_jefferson_1801.txt: ['thomas', 'jefferson', 'friends', 'and', 'fellow', 'citizens', 'called', 'upon', 'to', 'undertake']\n",
      "Stemmed tokens from 04_jefferson_1801.txt: ['thoma', 'jefferson', 'friend', 'and', 'fellow', 'citizen', 'call', 'upon', 'to', 'undertak']\n",
      "Tokens from 05_jefferson_1805.txt: ['thomas', 'jefferson', 'proceeding', 'fellow', 'citizens', 'to', 'that', 'qualification', 'which', 'the']\n",
      "Stemmed tokens from 05_jefferson_1805.txt: ['thoma', 'jefferson', 'proceed', 'fellow', 'citizen', 'to', 'that', 'qualif', 'which', 'the']\n",
      "Tokens from 06_madison_1809.txt: ['james', 'madison', 'unwilling', 'to', 'depart', 'from', 'examples', 'of', 'the', 'most']\n",
      "Stemmed tokens from 06_madison_1809.txt: ['jame', 'madison', 'unwil', 'to', 'depart', 'from', 'exampl', 'of', 'the', 'most']\n",
      "Tokens from 07_madison_1813.txt: ['james', 'madison', 'about', 'to', 'add', 'the', 'solemnity', 'of', 'an', 'oath']\n",
      "Stemmed tokens from 07_madison_1813.txt: ['jame', 'madison', 'about', 'to', 'add', 'the', 'solemn', 'of', 'an', 'oath']\n",
      "Tokens from 08_monroe_1817.txt: ['james', 'monroe', 'i', 'should', 'be', 'destitute', 'of', 'feeling', 'if', 'i']\n",
      "Stemmed tokens from 08_monroe_1817.txt: ['jame', 'monro', 'i', 'should', 'be', 'destitut', 'of', 'feel', 'if', 'i']\n",
      "Tokens from 09_monroe_1821.txt: ['james', 'monroe', 'fellow', 'citizens', 'i', 'shall', 'not', 'attempt', 'to', 'describe']\n",
      "Stemmed tokens from 09_monroe_1821.txt: ['jame', 'monro', 'fellow', 'citizen', 'i', 'shall', 'not', 'attempt', 'to', 'describ']\n",
      "Tokens from 10_adams_john_quincy_1825.txt: ['john', 'quincy', 'adams', 'in', 'compliance', 'with', 'an', 'usage', 'coeval', 'with']\n",
      "Stemmed tokens from 10_adams_john_quincy_1825.txt: ['john', 'quinci', 'adam', 'in', 'complianc', 'with', 'an', 'usag', 'coeval', 'with']\n",
      "Tokens from 11_jackson_1829.txt: ['andrew', 'jackson', 'fellow', 'citizens', 'about', 'to', 'undertake', 'the', 'arduous', 'duties']\n",
      "Stemmed tokens from 11_jackson_1829.txt: ['andrew', 'jackson', 'fellow', 'citizen', 'about', 'to', 'undertak', 'the', 'arduou', 'duti']\n",
      "Tokens from 12_jackson_1833.txt: ['andrew', 'jackson', 'the', 'will', 'of', 'the', 'american', 'people', 'expressed', 'through']\n",
      "Stemmed tokens from 12_jackson_1833.txt: ['andrew', 'jackson', 'the', 'will', 'of', 'the', 'american', 'peopl', 'express', 'through']\n",
      "Tokens from 13_van_buren_1837.txt: ['martin', 'van', 'buren', 'fellow', 'citizens', 'the', 'practice', 'of', 'all', 'my']\n",
      "Stemmed tokens from 13_van_buren_1837.txt: ['martin', 'van', 'buren', 'fellow', 'citizen', 'the', 'practic', 'of', 'all', 'my']\n",
      "Tokens from 14_harrison_1841.txt: ['william', 'henry', 'harrison', 'called', 'from', 'a', 'retirement', 'which', 'i', 'had']\n",
      "Stemmed tokens from 14_harrison_1841.txt: ['william', 'henri', 'harrison', 'call', 'from', 'a', 'retir', 'which', 'i', 'had']\n",
      "Tokens from 15_polk_1845.txt: ['james', 'k', 'polk', 'fellow', 'citizens', 'without', 'solicitation', 'on', 'my', 'part']\n",
      "Stemmed tokens from 15_polk_1845.txt: ['jame', 'k', 'polk', 'fellow', 'citizen', 'without', 'solicit', 'on', 'my', 'part']\n",
      "Tokens from 16_taylor_1849.txt: ['zachary', 'taylor', 'elected', 'by', 'the', 'american', 'people', 'to', 'the', 'highest']\n",
      "Stemmed tokens from 16_taylor_1849.txt: ['zachari', 'taylor', 'elect', 'by', 'the', 'american', 'peopl', 'to', 'the', 'highest']\n",
      "Tokens from 17_pierce_1853.txt: ['franklin', 'pierce', 'my', 'countrymen', 'it', 'is', 'a', 'relief', 'to', 'feel']\n",
      "Stemmed tokens from 17_pierce_1853.txt: ['franklin', 'pierc', 'my', 'countrymen', 'it', 'is', 'a', 'relief', 'to', 'feel']\n",
      "Tokens from 18_buchanan_1857.txt: ['james', 'buchanan', 'fellow', 'citizens', 'i', 'appear', 'before', 'you', 'this', 'day']\n",
      "Stemmed tokens from 18_buchanan_1857.txt: ['jame', 'buchanan', 'fellow', 'citizen', 'i', 'appear', 'befor', 'you', 'thi', 'day']\n",
      "Tokens from 19_lincoln_1861.txt: ['abraham', 'lincoln', 'fellow', 'citizens', 'of', 'the', 'united', 'states', 'in', 'compliance']\n",
      "Stemmed tokens from 19_lincoln_1861.txt: ['abraham', 'lincoln', 'fellow', 'citizen', 'of', 'the', 'unit', 'state', 'in', 'complianc']\n",
      "Tokens from 20_lincoln_1865.txt: ['abraham', 'lincoln', 'fellow', 'countrymen', 'at', 'this', 'second', 'appearing', 'to', 'take']\n",
      "Stemmed tokens from 20_lincoln_1865.txt: ['abraham', 'lincoln', 'fellow', 'countrymen', 'at', 'thi', 'second', 'appear', 'to', 'take']\n",
      "Tokens from 21_grant_1869.txt: ['ulysses', 's', 'grant', 'citizens', 'of', 'the', 'united', 'states', 'your', 'suffrages']\n",
      "Stemmed tokens from 21_grant_1869.txt: ['ulyss', 's', 'grant', 'citizen', 'of', 'the', 'unit', 'state', 'your', 'suffrag']\n",
      "Tokens from 22_grant_1873.txt: ['ulysses', 's', 'grant', 'fellow', 'citizens', 'under', 'providence', 'i', 'have', 'been']\n",
      "Stemmed tokens from 22_grant_1873.txt: ['ulyss', 's', 'grant', 'fellow', 'citizen', 'under', 'provid', 'i', 'have', 'been']\n",
      "Tokens from 23_hayes_1877.txt: ['rutherford', 'b', 'hayes', 'fellow', 'citizens', 'we', 'have', 'assembled', 'to', 'repeat']\n",
      "Stemmed tokens from 23_hayes_1877.txt: ['rutherford', 'b', 'hay', 'fellow', 'citizen', 'we', 'have', 'assembl', 'to', 'repeat']\n",
      "Tokens from 24_garfield_1881.txt: ['james', 'a', 'garfield', 'fellow', 'citizens', 'we', 'stand', 'to', 'day', 'upon']\n",
      "Stemmed tokens from 24_garfield_1881.txt: ['jame', 'a', 'garfield', 'fellow', 'citizen', 'we', 'stand', 'to', 'day', 'upon']\n",
      "Tokens from 25_cleveland_1885.txt: ['grover', 'cleveland', 'fellow', 'citizens', 'in', 'the', 'presence', 'of', 'this', 'vast']\n",
      "Stemmed tokens from 25_cleveland_1885.txt: ['grover', 'cleveland', 'fellow', 'citizen', 'in', 'the', 'presenc', 'of', 'thi', 'vast']\n",
      "Tokens from 26_harrison_1889.txt: ['benjamin', 'harrison', 'fellow', 'citizens', 'there', 'is', 'no', 'constitutional', 'or', 'legal']\n",
      "Stemmed tokens from 26_harrison_1889.txt: ['benjamin', 'harrison', 'fellow', 'citizen', 'there', 'is', 'no', 'constitut', 'or', 'legal']\n",
      "Tokens from 27_cleveland_1893.txt: ['grover', 'cleveland', 'my', 'fellow', 'citizens', 'in', 'obedience', 'of', 'the', 'mandate']\n",
      "Stemmed tokens from 27_cleveland_1893.txt: ['grover', 'cleveland', 'my', 'fellow', 'citizen', 'in', 'obedi', 'of', 'the', 'mandat']\n",
      "Tokens from 28_mckinley_1897.txt: ['william', 'mckinley', 'fellow', 'citizens', 'in', 'obedience', 'to', 'the', 'will', 'of']\n",
      "Stemmed tokens from 28_mckinley_1897.txt: ['william', 'mckinley', 'fellow', 'citizen', 'in', 'obedi', 'to', 'the', 'will', 'of']\n",
      "Tokens from 29_mckinley_1901.txt: ['william', 'mckinley', 'my', 'fellow', 'citizens', 'when', 'we', 'assembled', 'here', 'on']\n",
      "Stemmed tokens from 29_mckinley_1901.txt: ['william', 'mckinley', 'my', 'fellow', 'citizen', 'when', 'we', 'assembl', 'here', 'on']\n",
      "Tokens from 30_roosevelt_theodore_1905.txt: ['theodore', 'roosevelt', 'my', 'fellow', 'citizens', 'no', 'people', 'on', 'earth', 'have']\n",
      "Stemmed tokens from 30_roosevelt_theodore_1905.txt: ['theodor', 'roosevelt', 'my', 'fellow', 'citizen', 'no', 'peopl', 'on', 'earth', 'have']\n",
      "Tokens from 31_taft_1909.txt: ['william', 'howard', 'taft', 'my', 'fellow', 'citizens', 'anyone', 'who', 'has', 'taken']\n",
      "Stemmed tokens from 31_taft_1909.txt: ['william', 'howard', 'taft', 'my', 'fellow', 'citizen', 'anyon', 'who', 'ha', 'taken']\n",
      "Tokens from 32_wilson_1913.txt: ['woodrow', 'wilson', 'there', 'has', 'been', 'a', 'change', 'of', 'government', 'it']\n",
      "Stemmed tokens from 32_wilson_1913.txt: ['woodrow', 'wilson', 'there', 'ha', 'been', 'a', 'chang', 'of', 'govern', 'it']\n",
      "Tokens from 33_wilson_1917.txt: ['woodrow', 'wilson', 'the', 'four', 'years', 'which', 'have', 'elapsed', 'since', 'last']\n",
      "Stemmed tokens from 33_wilson_1917.txt: ['woodrow', 'wilson', 'the', 'four', 'year', 'which', 'have', 'elaps', 'sinc', 'last']\n",
      "Tokens from 34_harding_1921.txt: ['warren', 'g', 'harding', 'when', 'one', 'surveys', 'the', 'world', 'about', 'him']\n",
      "Stemmed tokens from 34_harding_1921.txt: ['warren', 'g', 'hard', 'when', 'one', 'survey', 'the', 'world', 'about', 'him']\n",
      "Tokens from 35_coolidge_1925.txt: ['calvin', 'coolidge', 'my', 'countrymen', 'no', 'one', 'can', 'contemplate', 'current', 'conditions']\n",
      "Stemmed tokens from 35_coolidge_1925.txt: ['calvin', 'coolidg', 'my', 'countrymen', 'no', 'one', 'can', 'contempl', 'current', 'condit']\n",
      "Tokens from 36_hoover_1929.txt: ['herbert', 'hoover', 'delivered', 'in', 'person', 'at', 'the', 'capitol', 'my', 'countrymen']\n",
      "Stemmed tokens from 36_hoover_1929.txt: ['herbert', 'hoover', 'deliv', 'in', 'person', 'at', 'the', 'capitol', 'my', 'countrymen']\n",
      "Tokens from 37_roosevelt_franklin_1933.txt: ['franklin', 'd', 'roosevelt', 'i', 'am', 'certain', 'that', 'my', 'fellow', 'americans']\n",
      "Stemmed tokens from 37_roosevelt_franklin_1933.txt: ['franklin', 'd', 'roosevelt', 'i', 'am', 'certain', 'that', 'my', 'fellow', 'american']\n",
      "Tokens from 38_roosevelt_franklin_1937.txt: ['franklin', 'd', 'roosevelt', 'when', 'four', 'years', 'ago', 'we', 'met', 'to']\n",
      "Stemmed tokens from 38_roosevelt_franklin_1937.txt: ['franklin', 'd', 'roosevelt', 'when', 'four', 'year', 'ago', 'we', 'met', 'to']\n",
      "Tokens from 39_roosevelt_franklin_1941.txt: ['franklin', 'd', 'roosevelt', 'on', 'each', 'national', 'day', 'of', 'inauguration', 'since']\n",
      "Stemmed tokens from 39_roosevelt_franklin_1941.txt: ['franklin', 'd', 'roosevelt', 'on', 'each', 'nation', 'day', 'of', 'inaugur', 'sinc']\n",
      "Tokens from 40_roosevelt_franklin_1945.txt: ['franklin', 'd', 'roosevelt', 'mr', 'chief', 'justice', 'mr', 'vice', 'president', 'my']\n",
      "Stemmed tokens from 40_roosevelt_franklin_1945.txt: ['franklin', 'd', 'roosevelt', 'mr', 'chief', 'justic', 'mr', 'vice', 'presid', 'my']\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpusroot = './US_Inaugural_Addresses'\n",
    "file_count = 0\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for filename in os.listdir(corpusroot):\n",
    "    if filename.startswith('0') or filename.startswith('1') or filename.startswith('2') or filename.startswith('3') or filename.startswith('4'):\n",
    "        file = open(os.path.join(corpusroot, filename), \"r\", encoding='windows-1252')\n",
    "        doc = file.read()\n",
    "        doc = doc.lower()\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        print(f'Tokens from {filename}: {tokens[:10]}') #Printing only first 10 tokens \n",
    "        print(f'Stemmed tokens from {filename}: {stemmed_tokens[:10]}') #Printing only first 10 stemmed tokens to verify \n",
    "        file.close() \n",
    "        file_count += 1\n",
    "print(file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Using the tokens, we would like to compute the TF-IDF vector for each document. Given a query string, we can also calculate the query vector and calcuate similarity.\n",
    "\n",
    "In the class, we learned that we can use different weightings for queries and documents and the possible choices are shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'weighting_scheme.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notation of a weighting scheme is as follows: ddd.qqq, where ddd denotes the combination used for document vector and qqq denotes the combination used for query vector.\n",
    "\n",
    "A very standard weighting scheme is: ltc.lnc; where the processing for document and query vectors are as follows:\n",
    "Document: logarithmic tf, logarithmic idf, cosine normalization\n",
    "Query: logarithmic tf, no idf, cosine normalization\n",
    "\n",
    "Implement query-document similarity using the <b>ltc.lnc</b> weighting scheme and show the outputs for the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.12f\" % getidf('democracy'))\n",
    "print(\"%.12f\" % getidf('foreign'))\n",
    "print(\"%.12f\" % getidf('states'))\n",
    "print(\"%.12f\" % getidf('honor'))\n",
    "print(\"%.12f\" % getidf('great'))\n",
    "print(\"--------------\")\n",
    "print(\"%.12f\" % getweight('19_lincoln_1861.txt','constitution'))\n",
    "print(\"%.12f\" % getweight('23_hayes_1877.txt','public'))\n",
    "print(\"%.12f\" % getweight('25_cleveland_1885.txt','citizen'))\n",
    "print(\"%.12f\" % getweight('09_monroe_1821.txt','revenue'))\n",
    "print(\"%.12f\" % getweight('37_roosevelt_franklin_1933.txt','leadership'))\n",
    "print(\"--------------\")\n",
    "print(\"(%s, %.12f)\" % query(\"states laws\"))\n",
    "print(\"(%s, %.12f)\" % query(\"war offenses\"))\n",
    "print(\"(%s, %.12f)\" % query(\"british war\"))\n",
    "print(\"(%s, %.12f)\" % query(\"texas government\"))\n",
    "print(\"(%s, %.12f)\" % query(\"world civilization\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to Submit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Submit through Canvas your source code in a single .py file.</b> You can use any standard Python library. The only non-standard library/package allowed for this assignment is NLTK. You .py file must define at least the following functions:\n",
    "\n",
    "* getidf(token): return the inverse document frequency of a token. If the token doesn't exist in the corpus, return -1. You should stem the parameter 'token' before calculating the idf score.\n",
    "\n",
    "* getweight(filename,token): return the normalized TF-IDF weight of a token in the document named 'filename'. If the token doesn't exist in the document, return 0. You should stem the parameter 'token' before calculating the tf-idf score.\n",
    "\n",
    "* query(qstring): return a tuple in the form of (filename of the document, score), where the document is the query answer with respect to the weighting scheme. You should stem the query tokens before calculating similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your program will be evaluated using the following criteria: \n",
    "\n",
    "* Correctness (75 Points)\n",
    "\n",
    "We will evaluate your code by calling the functions specificed above (getidf - 20 points; getweight - 25 points; query - 30 points). So, make sure to use the same function names, parameter names/types/orders as specified above. We will use the above test cases and other queries and tokens to test your program.\n",
    "\n",
    "\n",
    "* Preprocessing, Efficiency, modularity (25 Points)\n",
    "\n",
    "You should correctly follow the preprocessing steps. An efficient solution should be able to answer a query in a few seconds, you will get deductions if you code takes too long to run (>1 minute). Also, it should consider the boundary cases. Your program should behave correctly under special cases and even incorrect input. Follow good coding standards to make your program easy to understand by others and easy to maintain/extend. \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
